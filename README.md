# ðŸš€ DPO Fine-Tuning on Qwen2.5-7B-Instruct
### Fine-tune the powerful Qwen2.5-7B-Instruct model using Direct Preference Optimization (DPO) on the Intel/orca_dpo_pairs dataset â€” with support for LoRA, 4-bit quantization, and W&B logging.

### ðŸ§  Overview
### This project modularizes the full DPO training pipeline using:

### ðŸ¤— Hugging Face Transformers, PEFT, and TRL

### ðŸ“¦ 4-bit quantization using bitsandbytes

### ðŸ”§ LoRA for efficient parameter tuning

### ðŸ“Š W&B for experiment tracking

### ðŸ§© Environment-variable-based configuration

### âœ… Designed to run on any machine (Kaggle-independent)

```bash
dpo_training_project/
â”œâ”€â”€ config/                # Auth and model configuration
â”œâ”€â”€ data/                  # Dataset loading and preprocessing
â”œâ”€â”€ training/              # Trainer setup
â”œâ”€â”€ utils/                 # Formatting logic (ChatML)
â”œâ”€â”€ main.py                # Main training & merge script
â”œâ”€â”€ .env                   # Your API keys (not committed)
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md              # You're here

```
