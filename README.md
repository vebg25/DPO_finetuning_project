# ðŸš€ DPO Fine-Tuning on Qwen2.5-7B-Instruct
### Fine-tune the powerful Qwen2.5-7B-Instruct model using Direct Preference Optimization (DPO) on the Intel/orca_dpo_pairs dataset â€” with support for LoRA, 4-bit quantization, and W&B logging.

### ðŸ§  Overview
### This project modularizes the full DPO training pipeline using:

### ðŸ¤— Hugging Face Transformers, PEFT, and TRL

### ðŸ“¦ 4-bit quantization using bitsandbytes

### ðŸ”§ LoRA for efficient parameter tuning

### ðŸ“Š W&B for experiment tracking

### ðŸ§© Environment-variable-based configuration

### âœ… Designed to run on any machine (Kaggle-independent)
