# ğŸš€ DPO Fine-Tuning on Qwen2.5-7B-Instruct
### Fine-tune the powerful Qwen2.5-7B-Instruct model using Direct Preference Optimization (DPO) on the Intel/orca_dpo_pairs dataset â€” with support for LoRA, 4-bit quantization, and W&B logging.

### ğŸ§  Features Overview
| Feature                   | Description                                                |
| ------------------------- | ---------------------------------------------------------- |
| ğŸ¤— Hugging Face Ecosystem | Uses Transformers, TRL, and PEFT for modern LLM finetuning |
| ğŸ“¦ Quantization           | Optimized with 4-bit `bitsandbytes` inference and training |
| ğŸ”§ Efficient Training     | Lightweight LoRA adaptation for cost-effective finetuning  |
| ğŸ“Š Experiment Tracking    | Integrated W\&B logging for robust model analysis          |
| ğŸ” Secure Configuration   | `.env` based secret management for API tokens              |
| ğŸ§© Modular Codebase       | Clean, scalable, and reusable module structure             |
| âœ… Fully Offline Friendly  | Runs on any local machine â€” **no Kaggle dependency**       |

### Project Structure
```bash
dpo_training_project/
â”œâ”€â”€ config/                # Auth and model configuration
â”œâ”€â”€ data/                  # Dataset loading and preprocessing
â”œâ”€â”€ training/              # Trainer setup
â”œâ”€â”€ utils/                 # Formatting logic (ChatML)
â”œâ”€â”€ main.py                # Main training & merge script
â”œâ”€â”€ .env                   # Your API keys (not committed)
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md              # You're here

```
### Clone this repository
```bash
git clone https://github.com/vebg25/DPO_finetuning_project.git
```
### Install the requirements.txt
```bash
pip install -r requirements.txt
```

### Put your credentials
```ini
WANDB_API_KEY=your_wandb_api_key
HF_TOKEN=your_huggingface_token
```

### Run the main app
```python
pyhton main.py
```
